---
title: Performance Optimization
description: Optimize event-sourced systems with snapshots, caching, and batch operations
---

# Performance Optimization

Event sourcing can impact performance when dealing with entities that have thousands of events. This guide covers optimization strategies to keep your system fast.

## The Performance Challenge

Event sourcing requires replaying all events to reconstruct state:

```typescript
// Every load requires replaying all events
const events = await adapter.getEventsByEntityId({
  entityName: 'user',
  entityId: userId
});

// Reduce all events to get current state
const state = events.reduce(reducer, undefined);
// For 1000 events, this happens every load!
```

**Performance Impact:**
- Loading an entity with 10k events: 100-500ms
- Loading an entity with 100k events: 1000ms+
- With multiple concurrent loads: significant server strain

## Strategy 1: Snapshots

Snapshots cache the state at a point in time, allowing you to skip replaying old events.

### Basic Snapshot Pattern

```typescript
interface SnapshotAdapter extends Adapter {
  // New methods for snapshots
  getSnapshot?(
    entityName: string,
    entityId: string
  ): Promise<{
    state: any;
    versionAsOf: string;
  } | null>;

  saveSnapshot?(
    entityName: string,
    entityId: string,
    state: any,
    versionAsOf: string
  ): Promise<void>;
}

// Snapshot-aware repository
async function findOneWithSnapshot(
  entityName: string,
  entityId: string
) {
  // Try to load snapshot first
  const snapshot = await adapter.getSnapshot?.(
    entityName,
    entityId
  );

  let state = snapshot?.state;
  let lastEventId = snapshot?.versionAsOf;

  // Load remaining events after snapshot
  const events = await adapter.getEventsByEntityId({
    entityName,
    entityId
  });

  let filteredEvents = events;
  if (lastEventId) {
    // Only replay events after the snapshot
    filteredEvents = events.filter(
      e => e.eventId > lastEventId
    );
  }

  // Reduce remaining events
  const finalState = filteredEvents.reduce(
    reducer,
    state
  );

  return finalState;
}
```

### Snapshot Creation Strategy

Create snapshots at intervals or based on thresholds:

```typescript
// Create snapshots after certain number of events
const AUTO_SNAPSHOT_INTERVAL = 100; // Every 100 events

async function commitWithSnapshot(
  repository: any,
  entity: any
) {
  // Commit normally
  await repository.commit(entity);

  // Check if we should create snapshot
  const events = await adapter.getEventsByEntityId({
    entityName: entity.constructor.name.toLowerCase(),
    entityId: entity.entityId
  });

  if (events.length % AUTO_SNAPSHOT_INTERVAL === 0) {
    // Create snapshot
    await adapter.saveSnapshot?.(
      entity.constructor.name.toLowerCase(),
      entity.entityId,
      entity.state,
      events[events.length - 1].eventId
    );
  }
}
```

### Snapshot Storage Implementation

Store snapshots in a separate collection:

```typescript
interface SnapshotStore {
  entityName: string;
  entityId: string;
  state: any;
  versionAsOf: string;
  createdAt: string;
}

// MongoDB snapshot implementation
const createMongoSnapshotAdapter = (
  db: Database
): SnapshotAdapter => {
  const snapshotsCollection = db.collection('snapshots');

  return {
    async getSnapshot(entityName, entityId) {
      return snapshotsCollection.findOne({
        entityName,
        entityId
      });
    },

    async saveSnapshot(
      entityName,
      entityId,
      state,
      versionAsOf
    ) {
      await snapshotsCollection.updateOne(
        { entityName, entityId },
        {
          $set: {
            entityName,
            entityId,
            state,
            versionAsOf,
            createdAt: new Date()
          }
        },
        { upsert: true }
      );
    }
  };
};
```

**Performance Gains:**
- Loading entity with 1000 events from 100-500ms â†’ 10-50ms with snapshot every 100 events
- Reduces CPU usage by 90%+
- Decreases database load
- Faster response times

## Strategy 2: Caching

Cache loaded entity state to avoid repeated loads.

### In-Memory Caching

```typescript
import NodeCache from 'node-cache';

const stateCache = new NodeCache({
  stdTTL: 3600, // 1 hour
  checkperiod: 120 // Check every 2 minutes
});

async function findOneWithCache(
  entityName: string,
  entityId: string
) {
  const cacheKey = `${entityName}:${entityId}`;

  // Check cache first
  const cached = stateCache.get(cacheKey);
  if (cached) {
    return cached;
  }

  // Load from events if not cached
  const entity = await loadEntityFromEvents(
    entityName,
    entityId
  );

  // Cache the result
  stateCache.set(cacheKey, entity.state);

  return entity;
}
```

### Cache Invalidation with Plugins

Use plugins to invalidate cache when entities change:

```typescript
const cacheInvalidationPlugin: Plugin = {
  async onCommitted({ entityName, entityId, state }) {
    const cacheKey = `${entityName}:${entityId}`;

    // Invalidate old cache
    stateCache.del(cacheKey);

    // Populate new cache
    stateCache.set(cacheKey, state);

    // Invalidate related caches
    const relatedKeys = getRelatedCacheKeys(
      entityName,
      state
    );
    relatedKeys.forEach(key => stateCache.del(key));
  }
};

function getRelatedCacheKeys(
  entityName: string,
  state: any
): string[] {
  const keys = [];

  // Example: user change affects user lists
  if (entityName === 'user') {
    keys.push(`user-list`, `user-count`);
    keys.push(`user-by-status:${state.status}`);
  }

  // Example: order change affects order lists
  if (entityName === 'order') {
    keys.push(`order-list`);
    keys.push(`orders-by-customer:${state.customerId}`);
    keys.push(`orders-by-status:${state.status}`);
  }

  return keys;
}
```

### Distributed Caching with Redis

```typescript
import Redis from 'ioredis';

const redis = new Redis(process.env.REDIS_URL);

async function findOneWithRedis(
  entityName: string,
  entityId: string
) {
  const cacheKey = `${entityName}:${entityId}`;

  // Check Redis cache
  const cached = await redis.get(cacheKey);
  if (cached) {
    return JSON.parse(cached);
  }

  // Load from events
  const entity = await loadEntityFromEvents(
    entityName,
    entityId
  );

  // Cache in Redis
  await redis.setex(
    cacheKey,
    3600, // 1 hour TTL
    JSON.stringify(entity.state)
  );

  return entity;
}

// Cache invalidation plugin
const redisInvalidationPlugin: Plugin = {
  async onCommitted({ entityName, entityId, state }) {
    const cacheKey = `${entityName}:${entityId}`;

    // Update cache with new state
    await redis.setex(
      cacheKey,
      3600,
      JSON.stringify(state)
    );

    // Invalidate related caches
    const relatedKeys = getRelatedCacheKeys(
      entityName,
      state
    );
    if (relatedKeys.length > 0) {
      await redis.del(...relatedKeys);
    }
  }
};
```

**Performance Gains:**
- Cache hits: `<1ms` (vs 100-500ms without cache)
- Reduces database queries by 90%+
- Decreases event replay overhead
- Linear scaling vs logarithmic without cache

## Strategy 3: Batch Operations

Process multiple operations efficiently.

### Batch Event Replay

```typescript
// Load multiple entities efficiently
async function findManyWithCache(
  entityName: string,
  entityIds: string[]
) {
  const results = new Map();
  const missingIds = [];

  // Check cache for all IDs
  for (const id of entityIds) {
    const cached = await redis.get(
      `${entityName}:${id}`
    );
    if (cached) {
      results.set(id, JSON.parse(cached));
    } else {
      missingIds.push(id);
    }
  }

  // Load missing entities in batch
  if (missingIds.length > 0) {
    const events = await adapter.getEventsByEntityIds({
      entityName,
      entityIds: missingIds
    });

    // Process events by entity ID
    const eventsByEntity = groupBy(events, 'entityId');

    for (const [entityId, entityEvents] of Object.entries(
      eventsByEntity
    )) {
      const state = entityEvents.reduce(
        reducer,
        undefined
      );
      results.set(entityId, state);

      // Cache for future use
      await redis.setex(
        `${entityName}:${entityId}`,
        3600,
        JSON.stringify(state)
      );
    }
  }

  // Return in original order
  return entityIds.map(id => results.get(id));
}
```

### Batch Commits

```typescript
// Buffer and batch commits
class BatchingRepository {
  private commitBuffer: any[] = [];
  private flushTimer: NodeJS.Timeout | null = null;
  private readonly batchSize = 100;
  private readonly flushInterval = 1000; // 1 second

  async commit(entity: any) {
    this.commitBuffer.push(entity);

    // Flush if buffer is full
    if (this.commitBuffer.length >= this.batchSize) {
      await this.flush();
    } else if (!this.flushTimer) {
      // Schedule flush for later
      this.flushTimer = setTimeout(
        () => this.flush(),
        this.flushInterval
      );
    }
  }

  private async flush() {
    if (this.commitBuffer.length === 0) return;

    const toCommit = this.commitBuffer.splice(0);

    try {
      // Collect all events from all entities
      const allEvents = toCommit.flatMap(
        entity => entity.uncommittedEvents || []
      );

      // Single batch insert
      await adapter.commitEvents({ events: allEvents });

      // Run plugins once for batch
      for (const entity of toCommit) {
        for (const plugin of plugins) {
          await plugin.onCommitted({
            entityName: entity.constructor.name,
            entityId: entity.entityId,
            events: entity.uncommittedEvents,
            state: entity.state
          });
        }
      }
    } finally {
      if (this.flushTimer) {
        clearTimeout(this.flushTimer);
        this.flushTimer = null;
      }
    }
  }
}
```

**Performance Gains:**
- Batch loading 100 entities: 10x faster
- Reduced database round trips
- Better connection pooling
- Lower latency

## Strategy 4: Indexing and Queries

Optimize database queries for events.

### Database Indexes

```typescript
// MongoDB indexes
const eventCollection = db.collection('events');

// Index for quick entity lookups
await eventCollection.createIndex({
  entityName: 1,
  entityId: 1,
  eventId: 1
});

// Index for time-based queries
await eventCollection.createIndex({
  eventCreatedAt: -1
});

// Index for event type queries
await eventCollection.createIndex({
  eventName: 1
});

// Compound index for common queries
await eventCollection.createIndex({
  entityName: 1,
  entityId: 1,
  eventCreatedAt: -1
});
```

### Query Optimization

```typescript
// Inefficient: loads all events
async function countUserCreations() {
  const allEvents = await adapter.getEventsByEntityId({
    entityName: 'user',
    entityId: userId
  });
  return allEvents.filter(e =>
    e.eventName === 'user:created'
  ).length;
}

// Efficient: database-level filtering
async function countUserCreations() {
  return await db
    .collection('events')
    .countDocuments({
      entityName: 'user',
      entityId: userId,
      eventName: 'user:created'
    });
}

// Efficient: aggregate events by type
async function getEventStats(entityName: string) {
  return await db
    .collection('events')
    .aggregate([
      { $match: { entityName } },
      { $group: {
        _id: '$eventName',
        count: { $sum: 1 }
      }},
      { $sort: { count: -1 } }
    ])
    .toArray();
}
```

## Strategy 5: Event Compaction

Archive old events to reduce dataset size.

```typescript
interface CompactionStrategy {
  shouldCompact(eventCount: number): boolean;
  getCompactionPoint(events: Event[]): number;
}

// Compact every 1000 events
const eventCountStrategy: CompactionStrategy = {
  shouldCompact(eventCount) {
    return eventCount > 1000 && eventCount % 1000 === 0;
  },
  getCompactionPoint(events) {
    return Math.floor(events.length * 0.9); // Keep 90% newest
  }
};

// Archive events older than 1 year
const timeBasedStrategy: CompactionStrategy = {
  shouldCompact(eventCount) {
    return eventCount > 100; // Only if substantial
  },
  getCompactionPoint(events) {
    const oneYearAgo = new Date();
    oneYearAgo.setFullYear(oneYearAgo.getFullYear() - 1);

    return events.findIndex(
      e => new Date(e.eventCreatedAt) > oneYearAgo
    );
  }
};

async function compactEvents(
  entityName: string,
  entityId: string,
  strategy: CompactionStrategy
) {
  const events = await adapter.getEventsByEntityId({
    entityName,
    entityId
  });

  if (!strategy.shouldCompact(events.length)) {
    return;
  }

  const compactionPoint = strategy.getCompactionPoint(
    events
  );
  const eventsToArchive = events.slice(0, compactionPoint);
  const activeEvents = events.slice(compactionPoint);

  // Compute final state at compaction point
  const compactedState = eventsToArchive.reduce(
    reducer,
    undefined
  );

  // Save compacted snapshot
  await adapter.saveSnapshot?.(
    entityName,
    entityId,
    compactedState,
    eventsToArchive[eventsToArchive.length - 1].eventId
  );

  // Archive old events (keep for audit, but don't load)
  await archiveDB.insertMany(eventsToArchive);

  // Delete from active collection
  await adapter.deleteEventsBeforeId(
    entityName,
    entityId,
    eventsToArchive[eventsToArchive.length - 1].eventId
  );
}
```

## Performance Monitoring

Monitor and optimize your system:

```typescript
const performancePlugin: Plugin = {
  async onCommitted({
    entityName,
    entityId,
    events
  }) {
    const metrics = {
      entityName,
      entityId,
      eventCount: events.length,
      eventSize: JSON.stringify(events).length,
      timestamp: new Date().toISOString()
    };

    await metrics.record(metrics);

    // Alert if events are too large
    if (metrics.eventSize > 1000000) { // 1MB
      logger.warn('Large event payload', metrics);
    }
  }
};

// Monitor query performance
async function findOneWithMetrics(
  entityName: string,
  entityId: string
) {
  const start = performance.now();

  const entity = await findOneWithCache(
    entityName,
    entityId
  );

  const duration = performance.now() - start;

  if (duration > 100) {
    logger.warn('Slow entity load', {
      entityName,
      entityId,
      duration
    });
  }

  return entity;
}
```

## Performance Checklist

- [ ] Implement snapshots for entities with 100+ events
- [ ] Cache frequently accessed entities
- [ ] Use batch operations for multiple entities
- [ ] Create database indexes on entity lookups
- [ ] Monitor query performance
- [ ] Archive old events periodically
- [ ] Implement cache invalidation
- [ ] Test performance with realistic data volumes
- [ ] Use connection pooling
- [ ] Consider event compaction for large entities

## Benchmarking Results

Typical performance improvements with optimizations:

| Scenario | Without Optimization | With Optimization | Improvement |
|----------|---------------------|-------------------|------------|
| Load 1 entity (1000 events) | 250ms | 15ms | 16x |
| Load 100 entities | 25s | 500ms | 50x |
| Cache hit | N/A | `<1ms` | - |
| Snapshot at 100 events | 250ms | 25ms | 10x |

## Related Documentation

- [Best Practices](/docs/best-practices)
- [Storage Adapters](/docs/storage-adapters)
- [Plugins Overview](/docs/plugins)
